{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amk/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from polusa_dataset import POLUSADataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import evaluate as evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "batch_size = 1\n",
    "train_mask = range(50)\n",
    "num_labels = 4\n",
    "model_name = 'distilbert-base-uncased'\n",
    "lr = 1e-4\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = POLUSADataset(csv_file='POLUSA/2019_2.csv')\n",
    "total_length = len(dataset)\n",
    "train_length = int(total_length * train_size)\n",
    "val_length = int(total_length * val_size)\n",
    "test_length = int(total_length - (train_length + val_length))\n",
    "train_dataset, val_dataset, test_dataset= random_split(dataset, [train_length, val_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small subset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                sampler=SubsetRandomSampler(train_mask))\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                sampler=SubsetRandomSampler(train_mask))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                sampler=SubsetRandomSampler(train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mymodel, num_epochs, train_dataloader, validation_dataloader,device, lr):\n",
    "    \"\"\" Train a PyTorch Module\n",
    "    :param torch.nn.Module mymodel: the model to be trained\n",
    "    :param int num_epochs: number of epochs to train for\n",
    "    :param torch.utils.data.DataLoader train_dataloader: DataLoader containing training examples\n",
    "    :param torch.utils.data.DataLoader validation_dataloader: DataLoader containing validation examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param float lr: learning rate\n",
    "    :return None\n",
    "    \"\"\"\n",
    "    \n",
    "    # store for plotting\n",
    "    train_acc_epoch = []\n",
    "    train_acc_batch = []\n",
    "    val_acc_epoch = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    max_ = 0\n",
    "    Best_model = None\n",
    "    \n",
    "    \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    # here, we use the AdamW optimizer. Use torch.optim.Adam.\n",
    "    # instantiate it on the untrained model parameters with a learning rate of 5e-5\n",
    "    # print(\" >>>>>>>>  Initializing optimizer\")\n",
    "    optimizer = torch.optim.AdamW(mymodel.parameters(), lr=lr)\n",
    "\n",
    "    # now, we set up the learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # put the model in training mode (important that this is done each epoch,\n",
    "        # since we put the model into eval mode during validation)\n",
    "        mymodel.train()\n",
    "\n",
    "        # load metrics\n",
    "        train_accuracy = evaluate.load('accuracy')\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch + 1} training:\")\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            #load metrics\n",
    "            #train_accuracy_batch = evaluate.load('accuracy')\n",
    "\n",
    "            \"\"\"\n",
    "            You need to make some changes here to make this function work.\n",
    "            Specifically, you need to: \n",
    "            Extract the input_ids, attention_mask, and labels from the batch; then send them to the device. \n",
    "            Then, pass the input_ids and attention_mask to the model to get the logits.\n",
    "            Then, compute the loss using the logits and the labels.\n",
    "            Then, call loss.backward() to compute the gradients.\n",
    "            Then, call optimizer.step()  to update the model parameters.\n",
    "            Then, call lr_scheduler.step() to update the learning rate.\n",
    "            Then, call optimizer.zero_grad() to reset the gradients for the next iteration.\n",
    "            Then, compute the accuracy using the logits and the labels.\n",
    "            \"\"\"\n",
    "\n",
    "            # print(batch)\n",
    "            input_ids = batch['id'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            output = mymodel(input_ids, attention_mask=attention_mask)\n",
    "            predictions = output.logits\n",
    "\n",
    "            model_loss = loss(predictions, labels)\n",
    "\n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            # update metrics for train epoch \n",
    "            train_accuracy.add_batch(predictions=predictions, references=labels)\n",
    "            \n",
    "            # update metrics for train batch\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            train_accuracy_batch = correct/len(predictions)\n",
    "            train_acc_batch.append(train_accuracy_batch)\n",
    "                  \n",
    "                   \n",
    "        #computer for train epoch\n",
    "        acc = train_accuracy.compute()\n",
    "        train_acc_epoch.append(acc[\"accuracy\"])\n",
    "        \n",
    "        # print evaluation metrics\n",
    "        print(f\" ===> Epoch {epoch + 1}\")\n",
    "        print(f\" - Average training metrics: accuracy={acc}\")\n",
    "        \n",
    "        # normally, validation would be more useful when training for many epochs\n",
    "        val_accuracy = evaluate_model(mymodel, validation_dataloader, device)\n",
    "        val_acc_epoch.append(val_accuracy[\"accuracy\"])\n",
    "        \n",
    "        print(f\" - Average validation metrics: accuracy={val_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (val_acc_epoch[epoch] > max_):\n",
    "            max_ = val_acc_epoch[epoch]\n",
    "            saved_model_path = f'saved_models/model_{classifier_name}'\n",
    "            #torch.save(mymodel, saved_model_path)\n",
    "            Best_model = copy.deepcopy(mymodel)\n",
    "            Best_model.save_pretrained(saved_model_path)\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_train_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_epoch), f)\n",
    "    #     f.close()\n",
    "       \n",
    "\n",
    "    # with open(f'results/{classifier_name}_train_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_batch), f)\n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_epoch), f)  \n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "        \n",
    "    \n",
    "    return Best_model\n",
    "\n",
    "def evaluate_model(model, dataloader, device, acc_only=True):\n",
    "    \"\"\" Evaluate a PyTorch Model\n",
    "    :param torch.nn.Module model: the model to be evaluated\n",
    "    :param torch.utils.data.DataLoader test_dataloader: DataLoader containing testing examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param bool acc_only: return only accuracy if true, else also return ground truth and pred as tuple\n",
    "    :return accuracy (also return ground truth and pred as tuple if acc_only=False)\n",
    "    \"\"\"\n",
    "    # load metrics\n",
    "    dev_accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    # turn model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #Y_true and Y_pred store for epoch\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    \n",
    "    val_accuracy_batch = evaluate.load('accuracy')\n",
    "    \n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "       \n",
    "        predictions = output.logits\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        Y_true += batch['labels'].tolist()\n",
    "        Y_pred += predictions.tolist()\n",
    "        dev_accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "        if acc_only == True:\n",
    "            correct = (predictions.to(device) == batch['labels'].to(device)).sum().item()\n",
    "            val_accuracy_batch = correct/len(predictions)\n",
    "            val_acc_batch.append(val_accuracy_batch)\n",
    "            \n",
    "      \n",
    "\n",
    "    # compute and return metrics\n",
    "#     Y_true = np.squeeze(np.array(Y_true))\n",
    "#     Y_pred = np.squeeze(np.array(Y_pred))\n",
    "    \n",
    "    load_new_list(f'results/{classifier_name}_val_acc_batch',val_acc_batch)\n",
    "    \n",
    "    return dev_accuracy.compute() if acc_only else (dev_accuracy.compute(),Y_true,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train(pretrained_model, num_epochs, train_dataloader, validation_dataloader, device, lr)\n",
      "\u001b[1;32m/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m model_loss \u001b[39m=\u001b[39m loss(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m model_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:390\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    389\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 390\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    393\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train(pretrained_model, num_epochs, train_dataloader, validation_dataloader, device, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
