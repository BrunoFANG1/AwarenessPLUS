{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from polusa_dataset import POLUSADataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import evaluate as evaluate\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda'\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "batch_size = 30\n",
    "train_mask = range(50)\n",
    "num_labels = 4\n",
    "model_name = 'distilbert-base-uncased'\n",
    "lr = 1e-4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = POLUSADataset(csv_file='./POLUSA/2019_2.csv')\n",
    "total_length = len(dataset)\n",
    "train_length = int(total_length * train_size)\n",
    "val_length = int(total_length * val_size)\n",
    "test_length = int(total_length - (train_length + val_length))\n",
    "train_dataset, val_dataset, test_dataset= random_split(dataset, [train_length, val_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small subset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "# validation_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "\n",
    "# all data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(\"Batch:\")\n",
    "#     print(\"ID: \", batch['id'])\n",
    "#     print(\"Article: \", batch['body'])\n",
    "#     print(\"hyperpartisan: \", batch['hyperpartisan'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(mymodel, num_epochs, train_dataloader, validation_dataloader,device, lr):\n",
    "    \"\"\" Train a PyTorch Module\n",
    "    :param torch.nn.Module mymodel: the model to be trained\n",
    "    :param int num_epochs: number of epochs to train for\n",
    "    :param torch.utils.data.DataLoader train_dataloader: DataLoader containing training examples\n",
    "    :param torch.utils.data.DataLoader validation_dataloader: DataLoader containing validation examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param float lr: learning rate\n",
    "    :return None\n",
    "    \"\"\"\n",
    "    \n",
    "    # store for plotting\n",
    "    train_acc_epoch = []\n",
    "    train_acc_batch = []\n",
    "    val_acc_epoch = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    max_ = 0\n",
    "    Best_model = None\n",
    "    \n",
    "    \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    # here, we use the AdamW optimizer. Use torch.optim.Adam.\n",
    "    # instantiate it on the untrained model parameters with a learning rate of 5e-5\n",
    "    # print(\" >>>>>>>>  Initializing optimizer\")\n",
    "    optimizer = torch.optim.AdamW(mymodel.parameters(), lr=lr)\n",
    "\n",
    "    # now, we set up the learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # put the model in training mode (important that this is done each epoch,\n",
    "        # since we put the model into eval mode during validation)\n",
    "        mymodel.train()\n",
    "\n",
    "        # load metrics\n",
    "        train_accuracy = evaluate.load('accuracy')\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch + 1} training:\")\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "            \n",
    "            #load metrics\n",
    "            #train_accuracy_batch = evaluate.load('accuracy')\n",
    "\n",
    "            \"\"\"\n",
    "            You need to make some changes here to make this function work.\n",
    "            Specifically, you need to: \n",
    "            Extract the input_ids, attention_mask, and labels from the batch; then send them to the device. \n",
    "            Then, pass the input_ids and attention_mask to the model to get the logits.\n",
    "            Then, compute the loss using the logits and the labels.\n",
    "            Then, call loss.backward() to compute the gradients.\n",
    "            Then, call optimizer.step()  to update the model parameters.\n",
    "            Then, call lr_scheduler.step() to update the learning rate.\n",
    "            Then, call optimizer.zero_grad() to reset the gradients for the next iteration.\n",
    "            Then, compute the accuracy using the logits and the labels.\n",
    "            \"\"\"\n",
    "\n",
    "            # print(batch)\n",
    "            input_ids = batch['id'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            output = mymodel(input_ids, attention_mask=attention_mask)\n",
    "            predictions = output.logits\n",
    "\n",
    "            model_loss = loss(predictions, labels)\n",
    "\n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            # update metrics for train epoch \n",
    "            train_accuracy.add_batch(predictions=predictions, references=labels)\n",
    "            \n",
    "            # update metrics for train batch\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            train_accuracy_batch = correct/len(predictions)\n",
    "            train_acc_batch.append(train_accuracy_batch)\n",
    "                  \n",
    "                   \n",
    "        #computer for train epoch\n",
    "        acc = train_accuracy.compute()\n",
    "        train_acc_epoch.append(acc[\"accuracy\"])\n",
    "        \n",
    "        # print evaluation metrics\n",
    "        print(f\" ===> Epoch {epoch + 1}\")\n",
    "        print(f\" - Average training metrics: accuracy={acc}\")\n",
    "        \n",
    "        # normally, validation would be more useful when training for many epochs\n",
    "        val_accuracy = evaluate_model(mymodel, validation_dataloader, device)\n",
    "        val_acc_epoch.append(val_accuracy[\"accuracy\"])\n",
    "        \n",
    "        print(f\" - Average validation metrics: accuracy={val_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (val_acc_epoch[epoch] > max_):\n",
    "            max_ = val_acc_epoch[epoch]\n",
    "            saved_model_path = f'./saved_models/leaning/'#model_M2.1_leaning_DistilBERT_classifier'\n",
    "            torch.save(mymodel, f\"{saved_model_path}model_M2.1_leaning_DistillBert_Classifier.pt\")\n",
    "            Best_model = copy.deepcopy(mymodel)\n",
    "            Best_model.save_pretrained(saved_model_path)\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_train_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_epoch), f)\n",
    "    #     f.close()\n",
    "       \n",
    "\n",
    "    # with open(f'results/{classifier_name}_train_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_batch), f)\n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_epoch), f)  \n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "\n",
    "    \n",
    "    plt.plot(train_acc_epoch, '-o', label = 'training accuracy', color = 'blue')\n",
    "    plt.plot(val_acc_epoch, '-o', label = 'validation accuracy', color = 'orange')    \n",
    "    plt.title(\"Training and Validation Accuracies\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return Best_model\n",
    "\n",
    "def evaluate_model(model, dataloader, device, acc_only=True):\n",
    "    \"\"\" Evaluate a PyTorch Model\n",
    "    :param torch.nn.Module model: the model to be evaluated\n",
    "    :param torch.utils.data.DataLoader test_dataloader: DataLoader containing testing examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param bool acc_only: return only accuracy if true, else also return ground truth and pred as tuple\n",
    "    :return accuracy (also return ground truth and pred as tuple if acc_only=False)\n",
    "    \"\"\"\n",
    "    # load metrics\n",
    "    dev_accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    # turn model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #Y_true and Y_pred store for epoch\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    \n",
    "    val_accuracy_batch = evaluate.load('accuracy')\n",
    "    \n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['id'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "       \n",
    "        predictions = output.logits\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        Y_true += batch['labels'].tolist()\n",
    "        Y_pred += predictions.tolist()\n",
    "        dev_accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "        if acc_only == True:\n",
    "            correct = (predictions.to(device) == batch['labels'].to(device)).sum().item()\n",
    "            val_accuracy_batch = correct/len(predictions)\n",
    "            val_acc_batch.append(val_accuracy_batch)\n",
    "            \n",
    "      \n",
    "\n",
    "    # compute and return metrics\n",
    "#     Y_true = np.squeeze(np.array(Y_true))\n",
    "#     Y_pred = np.squeeze(np.array(Y_pred))\n",
    "    \n",
    "#     load_new_list(f'results/{classifier_name}_val_acc_batch',val_acc_batch)\n",
    "    \n",
    "    return dev_accuracy.compute() if acc_only else (dev_accuracy.compute(),Y_true,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [07:26<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===> Epoch 1\n",
      " - Average training metrics: accuracy={'accuracy': 0.8758423473828174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 168/168 [00:22<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Average validation metrics: accuracy={'accuracy': 0.9361321130123359}\n",
      "Epoch 2 training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [07:23<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===> Epoch 2\n",
      " - Average training metrics: accuracy={'accuracy': 0.9481536740022379}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 168/168 [00:22<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Average validation metrics: accuracy={'accuracy': 0.9307600477516912}\n",
      "Epoch 3 training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [07:33<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===> Epoch 3\n",
      " - Average training metrics: accuracy={'accuracy': 0.9699863235111277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 168/168 [00:22<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Average validation metrics: accuracy={'accuracy': 0.9468762435336251}\n",
      "Epoch 4 training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████▎                                                   | 464/1341 [02:37<04:51,  3.01it/s]"
     ]
    }
   ],
   "source": [
    "trained_model = train(pretrained_model, num_epochs, train_dataloader, validation_dataloader, device, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
