{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amk/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from polusa_dataset import POLUSADataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import evaluate as evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda'\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "batch_size = 1\n",
    "train_mask = range(50)\n",
    "num_labels = 4\n",
    "model_name = 'distilbert-base-uncased'\n",
    "lr = 1e-4\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = POLUSADataset(csv_file='./POLUSA/2019_2.csv')\n",
    "total_length = len(dataset)\n",
    "train_length = int(total_length * train_size)\n",
    "val_length = int(total_length * val_size)\n",
    "test_length = int(total_length - (train_length + val_length))\n",
    "train_dataset, val_dataset, test_dataset= random_split(dataset, [train_length, val_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small subset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "# validation_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                 sampler=SubsetRandomSampler(train_mask))\n",
    "\n",
    "# all data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mymodel, num_epochs, train_dataloader, validation_dataloader,device, lr):\n",
    "    \"\"\" Train a PyTorch Module\n",
    "    :param torch.nn.Module mymodel: the model to be trained\n",
    "    :param int num_epochs: number of epochs to train for\n",
    "    :param torch.utils.data.DataLoader train_dataloader: DataLoader containing training examples\n",
    "    :param torch.utils.data.DataLoader validation_dataloader: DataLoader containing validation examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param float lr: learning rate\n",
    "    :return None\n",
    "    \"\"\"\n",
    "    \n",
    "    # store for plotting\n",
    "    train_acc_epoch = []\n",
    "    train_acc_batch = []\n",
    "    val_acc_epoch = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    max_ = 0\n",
    "    Best_model = None\n",
    "    \n",
    "    \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    # here, we use the AdamW optimizer. Use torch.optim.Adam.\n",
    "    # instantiate it on the untrained model parameters with a learning rate of 5e-5\n",
    "    # print(\" >>>>>>>>  Initializing optimizer\")\n",
    "    optimizer = torch.optim.AdamW(mymodel.parameters(), lr=lr)\n",
    "\n",
    "    # now, we set up the learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # put the model in training mode (important that this is done each epoch,\n",
    "        # since we put the model into eval mode during validation)\n",
    "        mymodel.train()\n",
    "\n",
    "        # load metrics\n",
    "        train_accuracy = evaluate.load('accuracy')\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch + 1} training:\")\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            #load metrics\n",
    "            #train_accuracy_batch = evaluate.load('accuracy')\n",
    "\n",
    "            \"\"\"\n",
    "            You need to make some changes here to make this function work.\n",
    "            Specifically, you need to: \n",
    "            Extract the input_ids, attention_mask, and labels from the batch; then send them to the device. \n",
    "            Then, pass the input_ids and attention_mask to the model to get the logits.\n",
    "            Then, compute the loss using the logits and the labels.\n",
    "            Then, call loss.backward() to compute the gradients.\n",
    "            Then, call optimizer.step()  to update the model parameters.\n",
    "            Then, call lr_scheduler.step() to update the learning rate.\n",
    "            Then, call optimizer.zero_grad() to reset the gradients for the next iteration.\n",
    "            Then, compute the accuracy using the logits and the labels.\n",
    "            \"\"\"\n",
    "\n",
    "            # print(batch)\n",
    "            input_ids = batch['id'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            output = mymodel(input_ids, attention_mask=attention_mask)\n",
    "            predictions = output.logits\n",
    "\n",
    "            model_loss = loss(predictions, labels)\n",
    "\n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            # update metrics for train epoch \n",
    "            train_accuracy.add_batch(predictions=predictions, references=labels)\n",
    "            \n",
    "            # update metrics for train batch\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            train_accuracy_batch = correct/len(predictions)\n",
    "            train_acc_batch.append(train_accuracy_batch)\n",
    "                  \n",
    "                   \n",
    "        #computer for train epoch\n",
    "        acc = train_accuracy.compute()\n",
    "        train_acc_epoch.append(acc[\"accuracy\"])\n",
    "        \n",
    "        # print evaluation metrics\n",
    "        print(f\" ===> Epoch {epoch + 1}\")\n",
    "        print(f\" - Average training metrics: accuracy={acc}\")\n",
    "        \n",
    "        # normally, validation would be more useful when training for many epochs\n",
    "        val_accuracy = evaluate_model(mymodel, validation_dataloader, device)\n",
    "        val_acc_epoch.append(val_accuracy[\"accuracy\"])\n",
    "        \n",
    "        print(f\" - Average validation metrics: accuracy={val_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (val_acc_epoch[epoch] > max_):\n",
    "            max_ = val_acc_epoch[epoch]\n",
    "#             saved_model_path = f'saved_models/model_{classifier_name}'\n",
    "            #torch.save(mymodel, saved_model_path)\n",
    "#             Best_model = copy.deepcopy(mymodel)\n",
    "#             Best_model.save_pretrained(saved_model_path)\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_train_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_epoch), f)\n",
    "    #     f.close()\n",
    "       \n",
    "\n",
    "    # with open(f'results/{classifier_name}_train_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((train_acc_batch), f)\n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_epoch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_epoch), f)  \n",
    "    #     f.close()\n",
    "        \n",
    "        \n",
    "    # with open(f'results/{classifier_name}_val_acc_batch' + '.pickle', 'wb') as f:\n",
    "    #     pickle.dump((val_acc_batch), f)\n",
    "    #     f.close()\n",
    "        \n",
    "    \n",
    "    return Best_model\n",
    "\n",
    "def evaluate_model(model, dataloader, device, acc_only=True):\n",
    "    \"\"\" Evaluate a PyTorch Model\n",
    "    :param torch.nn.Module model: the model to be evaluated\n",
    "    :param torch.utils.data.DataLoader test_dataloader: DataLoader containing testing examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param bool acc_only: return only accuracy if true, else also return ground truth and pred as tuple\n",
    "    :return accuracy (also return ground truth and pred as tuple if acc_only=False)\n",
    "    \"\"\"\n",
    "    # load metrics\n",
    "    dev_accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    # turn model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #Y_true and Y_pred store for epoch\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    \n",
    "    val_accuracy_batch = evaluate.load('accuracy')\n",
    "    \n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['id'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "       \n",
    "        predictions = output.logits\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        Y_true += batch['labels'].tolist()\n",
    "        Y_pred += predictions.tolist()\n",
    "        dev_accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "        if acc_only == True:\n",
    "            correct = (predictions.to(device) == batch['labels'].to(device)).sum().item()\n",
    "            val_accuracy_batch = correct/len(predictions)\n",
    "            val_acc_batch.append(val_accuracy_batch)\n",
    "            \n",
    "      \n",
    "\n",
    "    # compute and return metrics\n",
    "#     Y_true = np.squeeze(np.array(Y_true))\n",
    "#     Y_pred = np.squeeze(np.array(Y_pred))\n",
    "    \n",
    "#     load_new_list(f'results/{classifier_name}_val_acc_batch',val_acc_batch)\n",
    "    \n",
    "    return dev_accuracy.compute() if acc_only else (dev_accuracy.compute(),Y_true,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train(pretrained_model, num_epochs, train_dataloader, validation_dataloader, device, lr)\n",
      "\u001b[1;32m/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(predictions, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# update metrics for train epoch \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m train_accuracy\u001b[39m.\u001b[39;49madd_batch(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# update metrics for train batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amk/school/ml_design/AwarenessPLUS/classification_pipeline.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m correct \u001b[39m=\u001b[39m (predictions \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/module.py:486\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(column) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    485\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_nested_string_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format[key], column[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 486\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselected_feature_format\u001b[39m.\u001b[39;49mencode_batch(batch)\n\u001b[1;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39mwrite_batch(batch)\n\u001b[1;32m    488\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid, \u001b[39mTypeError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/datasets/features/features.py:1857\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn mismatch between batch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mset\u001b[39m(batch)\u001b[39m}\u001b[39;00m\u001b[39m and features \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1856\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> 1857\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m   1858\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1859\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/datasets/features/features.py:439\u001b[0m, in \u001b[0;36mcast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcast_to_python_objects\u001b[39m(obj: Any, only_1d_for_numpy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, optimize_list_casting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    420\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39m    It works recursively.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m        casted_obj: the casted object\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m _cast_to_python_objects(\n\u001b[1;32m    440\u001b[0m         obj, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    441\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/datasets/features/features.py:316\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[()], \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_1d_for_numpy \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy(), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    319\u001b[0m         [\n\u001b[1;32m    320\u001b[0m             _cast_to_python_objects(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train(pretrained_model, num_epochs, train_dataloader, validation_dataloader, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
