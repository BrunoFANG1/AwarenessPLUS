{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e09f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import evaluate as evaluate\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d847dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda'\n",
    "\n",
    "batch_size = 1\n",
    "train_mask = range(50)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "leaning_model_dir = './saved_models/leaning/'\n",
    "hyperpartisan_model_dir = './saved_models/hyperpartisan/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1823ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as input_file:\n",
    "            self.data = json.load(input_file)\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    def __getitem__(self, idx):\n",
    "        x_token = self.tokenizer(self.data,\n",
    "                                 padding='max_length',\n",
    "                                 max_length=512,\n",
    "                                 truncation=True,\n",
    "                                 return_tensors='pt')        \n",
    "        return {'id':x_token['input_ids'][0], 'attention_mask':x_token['attention_mask'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e06e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArticleDataset(json_file='./input/input_article.json')\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa1fe6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, acc_only=True):\n",
    "    \"\"\" Evaluate a PyTorch Model\n",
    "    :param torch.nn.Module model: the model to be evaluated\n",
    "    :param torch.utils.data.DataLoader test_dataloader: DataLoader containing testing examples\n",
    "    :param torch.device device: the device that we'll be training on\n",
    "    :param bool acc_only: return only accuracy if true, else also return ground truth and pred as tuple\n",
    "    :return accuracy (also return ground truth and pred as tuple if acc_only=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #Y_true and Y_pred store for epoch\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    val_acc_batch = []\n",
    "    \n",
    "    \n",
    "    val_accuracy_batch = evaluate.load('accuracy')\n",
    "    \n",
    "    label = []\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['id'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "       \n",
    "        predictions = output.logits\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        label.append(predictions)\n",
    "        print(predictions)\n",
    "#         Y_true += batch['labels'].tolist()\n",
    "#         Y_pred += predictions.tolist()\n",
    "#         dev_accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "#         if acc_only == True:\n",
    "#             correct = (predictions.to(device) == batch['labels'].to(device)).sum().item()\n",
    "#             val_accuracy_batch = correct/len(predictions)\n",
    "#             val_acc_batch.append(val_accuracy_batch)\n",
    "            \n",
    "      \n",
    "\n",
    "    # compute and return metrics\n",
    "#     Y_true = np.squeeze(np.array(Y_true))\n",
    "#     Y_pred = np.squeeze(np.array(Y_pred))\n",
    "    \n",
    "#     load_new_list(f'results/{classifier_name}_val_acc_batch',val_acc_batch)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5584fcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leaning_model = AutoModelForSequenceClassification.from_pretrained(leaning_model_dir)\n",
    "hyperpartisan_model = AutoModelForSequenceClassification.from_pretrained(hyperpartisan_model_dir)\n",
    "hyperpartisan_model.to(device)\n",
    "leaning_model = AutoModelForSequenceClassification.from_pretrained(leaning_model_dir)\n",
    "leaning_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676b7c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# leaning_prediction = evaluate_model(leaning_model, dataloader, device)\n",
    "hyperpartisan_prediction = evaluate_model(hyperpartisan_model, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f2bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_hyperpartisan(label):\n",
    "    if label == 0:\n",
    "        return 'false'\n",
    "    elif label == 1:\n",
    "        return 'true'\n",
    "    \n",
    "def label_to_leaning(label):\n",
    "    if label == 0:\n",
    "        return 'left'\n",
    "    elif label == 1:\n",
    "        return 'right'\n",
    "    elif label == 2:\n",
    "        return 'center'\n",
    "    elif label == 3:\n",
    "        return 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffa26c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "print(label_to_hyperpartisan(int(hyperpartisan_prediction[0].data)))\n",
    "print(label_to_leaning(int(leaning_prediction[0].data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Political Leaning: {label_to_leaning(int(leaning_prediction[0].data))}\")\n",
    "print(f\"Is Hyperpartisan: {label_to_hyperpartisan(int(hyperpartisan_prediction[0].data))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
